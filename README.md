# ENG2112 - Machine Learning for Predictive Maintenanc

My names Finn

A structured Python implementation of machine learning models for predictive maintenance using the AI4I 2020 dataset. This project predicts machine failures using multiple classification strategies to handle extreme class imbalance.

## ğŸ“‹ Project Overview

**Dataset**: AI4I 2020 Predictive Maintenance Dataset  
**Task**: Binary classification of machine failures (~3.4% failure rate)  
**Features**: Sensor readings (temperature, speed, torque, tool wear)

## ğŸ—‚ï¸ Project Structure

```
ENG2112/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                    # Configuration settings
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preprocessing.py         # Data loading & preprocessing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ failure_predictor.py     # Model definitions
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py               # Evaluation functions
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ visualization.py         # Plotting utilities
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_failure_model.py       # Main training script
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ MillMate_G17_ENGG2112.ipynb  # Original notebook (reference)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ai4i2020.csv
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/                      # Saved models
â”‚   â”œâ”€â”€ results/                     # CSV results
â”‚   â””â”€â”€ figures/                     # Plots
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Verify Data

Ensure your dataset is at: `data/ai4i2020.csv`

### 3. Run Training Pipeline

```bash
python scripts/train_failure_model.py
```

This will:
- Load and preprocess the data
- Train 6 different model strategies
- Perform 5-fold cross-validation
- Evaluate on test set
- Generate performance visualizations
- Save results to `outputs/`

## ğŸ¯ Model Strategies

The project implements 6 strategies for handling class imbalance:

1. **BalancedRandomForestClassifier** â­ (Recommended)
2. **EasyEnsembleClassifier** â­ (Recommended)
3. **SMOTE + Logistic Regression**
4. **ADASYN + Logistic Regression**
5. **Class-Weighted Logistic Regression**
6. **Class-Weighted Random Forest**

## ğŸ“Š Evaluation Metrics

Models are evaluated using:
- **ROC-AUC**: Overall discriminative ability
- **PR-AUC**: Performance on imbalanced data (primary metric)
- **F1 Score**: Balance of precision and recall
- **MCC**: Matthews Correlation Coefficient
- **Precision & Recall**: At optimal threshold

## âš™ï¸ Configuration

Modify settings in `src/config.py`:

```python
# Key parameters
TEST_SIZE = 0.2           # Train/test split ratio
CV_FOLDS = 5              # Cross-validation folds
RANDOM_STATE = 42         # Random seed
THRESHOLD_PREFERENCE = 'f1'  # Options: 'f1', 'recall', 'precision'
```

## ğŸ“ˆ Outputs

After running the pipeline, check:

- **Results**: `outputs/results/failure_imbalance_comparison.csv`
- **Figures**: 
  - `outputs/figures/pr_curves_top3.png`
  - `outputs/figures/roc_curves_top3.png`
  - `outputs/figures/model_comparison.png`

## ğŸ”§ Usage Examples

### Run with Custom Configuration

```python
from src import config
from src.data.preprocessing import load_and_preprocess_data
from src.models.failure_predictor import create_failure_predictor

# Modify config as needed
config.TEST_SIZE = 0.3
config.CV_FOLDS = 10

# Run pipeline
X_train, X_test, y_train, y_test, preprocessor = load_and_preprocess_data(config)
predictor = create_failure_predictor(config, preprocessor)
```

### Evaluate a Single Strategy

```python
from src.evaluation.metrics import cross_validate_model, evaluate_on_test

# Get specific model
model = predictor.get_strategy('BalancedRF')

# Cross-validate
cv_results = cross_validate_model(model, X_train, y_train)
print(cv_results)

# Test evaluation
test_metrics, probs, preds = evaluate_on_test(
    model, X_train, y_train, X_test, y_test
)
```

## ğŸ§ª Testing

Run unit tests (when implemented):

```bash
python -m pytest tests/
```

## ğŸ“ Notes

- The original Jupyter notebook is preserved in `notebooks/` for reference
- For production use, consider saving trained models with `joblib` or `pickle`
- Threshold tuning is performed automatically based on `THRESHOLD_PREFERENCE`
- All random operations use fixed seeds for reproducibility

## ğŸ‘¥ Team

MillMate G17 Team - ENGG2112

## ğŸ“„ License

[Add your license here]

## ğŸ™ Acknowledgments

Dataset: AI4I 2020 Predictive Maintenance Dataset